# English version
(Chinese version below)
(This tool is developed by Student from Friedlich Alexander University Erlangen-Nuernberg, was inspired by FAUtv)

# What is this tool?

This tool combines the OpenAI-based **WhisperX** speech transcription tool with **DeepL API** or local/cloud-based large language models (LLMs) to efficiently generate multilingual subtitles.  
WhisperX: https://github.com/m-bain/whisperX

---

## **Target Audience**
1. International students (supports mainstream languages such as English, French, German, Spanish, Japanese, Korean, etc.).
2. Subtitle teams and related professionals.
3. Video content creators and uploaders.
4. Creators who need to make multilingual subtitles for videos.

---

## **Required Software**
- **Python IDE**: An IDE that supports running Python scripts, such as VSCode or PyCharm.
- **LLM Software**: Recommended to use LMStudio (also supports GPT4All and other alternatives).
- **Video Player**: Recommended to use PotPlayer or any video player that supports the .srt subtitle format.

---

## **Do you need programming experience?**
- Only basic programming knowledge is required.
- If you have no programming experience, you can quickly get started with tools like ChatGPT, Doubao, or Kimi.
- The steps are simple and easy to follow, no worries!
---
## **Configuration Options**

#### **Recommended Speech Recognition (for all configurations)**
- **Whisper's *medium* model**  
  â¡ï¸ Specially used for recognizing speech in videos, suitable for all configurations.

---

#### **Low-Configuration Computers**
- **Supported devices**:
  - ğŸ Apple M-series (or newer devices)
  - ğŸ’» Windows laptop with a 1050Ti GPU (tested and supported)

- **Optional settings**:
  - Use **DeepL API**  
    â¡ï¸ The free version supports approximately 15 hours of continuous speech-to-video translation per month. (Recommended)
  - Use **Llama 3.2 3B model**  
    â¡ï¸ Unlimited translation usage, but occasional translation errors may occur.

---

#### **High-Configuration Computers**
- **Tested device**:
  - ğŸ’» Device with **Nvidia 4060Ti 16GB VRAM**, smooth performance and excellent results.

- **Optional settings**:
  - Use **Llama 3.1 8B model** (Recommended)  
    â¡ï¸ High translation quality, supports unlimited free usage.
  - Use **ChatGPT API**  
    â¡ï¸ Best translation quality, but requires payment. For pricing details, refer to [ChatGPT API Pricing](https://chatgpt.com/#pricing).

## **Setup and Usage Steps**

### **1. Clone the project from GitHub**
- Clone the project to your local machine.

---

### **2. Configure the environment (Windows OS)**
1. Use VSCode and open the terminal in the project folder.
2. Enter the following commands:
   ```bash
   py -3.11 -m venv .venv
   .venv/Scripts/activate

3. Install the GPU version of PyTorch:
- Make sure the CUDA drivers are installed.
- Use the following command to install:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```
- If you donâ€™t have a GPU, install the CPU version:
   ```bash
   pip install torch torchvision torchaudio
   ```
4. Install all other required dependencies:
- Use the following command to install all necessary libraries:
   ```bash
   pip install -r requirements.txt
   ```
   **Note**: The installation process may take some time and require more than 2.5GB of storage space. You can grab a cup of coffee while it installs.

---

### **3. Place the video files**
- Put the video files you need to translate into the `Video_to_translate` folder.
- The translated subtitle files will be automatically saved in the `Generated_subtitles` folder.

---

### **4. Start the translation**
#### **Using DeepL API**
1. Open the `Autotranslate_Deepl_WhisperX.py` script.
2. Replace the `deepL_API` in line 7 with your DeepL API key.
3. Run the script and be patient. The first run may take some time to configure.

---

#### **Using Local LLM**
1. Download and install LMStudio software.
2. Download the Llama-3.2-3B-Instruct-Q8_0 model in LMStudio.
3. Follow these steps:
   Developer â†’ Choose the model to load â†’ Llama 3.2 3B Instruct â†’ Setting â†’ Check Server Port â†’ Enable CORS.
4. Modify the `Autotranslate_LLM_WhisperX.py` script and set the `LLM_MODEL` variable to the model name you downloaded in LMStudio.
5. Run the `Autotranslate_LLM_WhisperX.py` script.

---

### **5. Check the output**
- The subtitle files will be saved in the `Generated_subtitles` folder.
- Put the video file and subtitle file in the same folder and play the video with subtitles using PotPlayer or any compatible player.

---

---
## **Q&A Installation Issues**
### **1. Some warnings can be ignored**
   ```
   INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
   INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
   No language specified, language will be first be detected for each audio file (increases inference time).
   Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\Dean-Translator\.venv\Lib\site-packages\whisperx\assets\pytorch_model.bin`
   Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.
   Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.
   ```
### **2. Unable to install models after installing LMStudio**
   You can try this tool to [fix the script](https://github.com/yuanyang749/lm-studio-fix)  
   Or manually modify the script.

---

## **Q&A**

### **1. How accurate is the translation of technical terms?**
- Large language models generally produce highly accurate translations by leveraging context. 
- The DeepL API is reliable, though it may occasionally misinterpret synonyms.

---

### **2. How long does transcription take?**
- After initial setup, transcription time depends on your hardware and model choice.
- Higher-performance computers and smaller models reduce processing time.

---

### **3. How accurate are the subtitles?**
- Accuracy depends on:
  - **Audio quality**: Clearer audio results in better transcriptions.
  - **Transcription model performance**: Whisper's *large-v3* model provides the highest accuracy.
  - **Translation model quality**: Larger LLMs produce superior translations.

---

### **4. Supported languages?**
- This tool supports almost all languages supported by Whisper, DeepL, and large language models, including Chinese, Japanese, Korean, English, French, German, and more.

---

### **5. What are the minimum hardware requirements?**
- Tested on a MacBook Air M1 (8GB RAM) and an Nvidia 4060Ti GPU. Any computer with similar or better performance should work smoothly.

---

### **6. What video formats are supported?**
- Most mainstream video and audio formats are supported. Refer to Whisper's official documentation for more details.

---

# ä¸­æ–‡ç‰ˆ
(è¿™ä¸ªå·¥å…·ç”±åŸƒå°”æœ—æ ¹-çº½ä¼¦å ¡å¤§å­¦çš„å­¦ç”Ÿå¼€å‘ï¼Œå—åˆ°FAUtvçš„å¯å‘)
# è¿™ä¸ªå·¥å…·æ˜¯ä»€ä¹ˆï¼Ÿ
æœ¬å·¥å…·ç»“åˆäº†åŸºäºOpenAI çš„ **WhisperX** è¯­éŸ³è½¬å½•å·¥å…·ï¼Œé€šè¿‡ **DeepL API** æˆ–æœ¬åœ°/äº‘ç«¯çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œé«˜æ•ˆç”Ÿæˆå¤šè¯­è¨€å­—å¹•ã€‚
WhisperXï¼šhttps://github.com/m-bain/whisperX

---

## **é€‚ç”¨äººç¾¤**
1. å›½é™…å­¦ç”Ÿï¼ˆæ”¯æŒè‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ—¥è¯­ã€éŸ©è¯­ç­‰ä¸»æµè¯­è¨€ï¼‰ã€‚
2. å­—å¹•ç»„åŠç›¸å…³ä»ä¸šè€…ã€‚
3. è§†é¢‘æ¬è¿è€…åŠå†…å®¹åˆ›ä½œè€…ã€‚
4. éœ€è¦åˆ¶ä½œå¤šè¯­è¨€è§†é¢‘å­—å¹•çš„åˆ›ä½œè€…ã€‚

---

## **æ‰€éœ€è½¯ä»¶**
- **Python IDE**ï¼šæ”¯æŒè¿è¡Œ Python è„šæœ¬çš„ IDEï¼Œä¾‹å¦‚ VSCode æˆ– PyCharmã€‚
- **LLM è½¯ä»¶**ï¼šæ¨èä½¿ç”¨ LMStudioï¼ˆä¹Ÿæ”¯æŒ GPT4All ç­‰å…¶ä»–é€‰æ‹©ï¼‰ã€‚
- **è§†é¢‘æ’­æ”¾å™¨**ï¼šæ¨èä½¿ç”¨ PotPlayerï¼Œæˆ–ä»»ä½•æ”¯æŒ `.srt` å­—å¹•æ ¼å¼çš„è§†é¢‘æ’­æ”¾å™¨ã€‚

---

## **éœ€è¦ç¼–ç¨‹åŸºç¡€å—ï¼Ÿ**
- ä»…éœ€éå¸¸åŸºç¡€çš„ç¼–ç¨‹çŸ¥è¯†ã€‚
- å¦‚æœæ²¡æœ‰ç¼–ç¨‹ç»éªŒï¼Œå¯ä»¥å€ŸåŠ© ChatGPTã€è±†åŒ…æˆ– Kimi ç­‰å·¥å…·å¿«é€Ÿå…¥é—¨ã€‚
- æ­¥éª¤ç®€å•æ˜“æ‡‚ï¼Œæ— éœ€æ‹…å¿ƒï¼

---
## **é…ç½®é€‰é¡¹**

---

#### **è¯­éŸ³è¯†åˆ«æ¨èï¼ˆé€‚ç”¨äºæ‰€æœ‰é…ç½®ï¼‰**
- **Whisper çš„ *medium* æ¨¡å‹**  
  â¡ï¸ ä¸“é—¨ç”¨äºè¯†åˆ«è§†é¢‘ä¸­çš„è¯­éŸ³ï¼Œé€‚ç”¨äºæ‰€æœ‰é…ç½®ã€‚

---

#### **ä½é…ç½®ç”µè„‘**
- **æ”¯æŒè®¾å¤‡**ï¼š
  - ğŸ è‹¹æœ M ç³»åˆ—ï¼ˆæˆ–æ›´æ–°çš„è®¾å¤‡ï¼‰
  - ğŸ’» é…å¤‡ 1050Ti æ˜¾å¡çš„ Windows ç¬”è®°æœ¬ï¼ˆå·²æµ‹è¯•æ”¯æŒï¼‰
  
- **å¯é€‰è®¾ç½®**ï¼š
  - ä½¿ç”¨ **DeepL API**  
    â¡ï¸ å…è´¹ç‰ˆæ¯æœˆæ”¯æŒçº¦ 15 å°æ—¶çš„è¿ç»­è¯­éŸ³è§†é¢‘ç¿»è¯‘ã€‚ï¼ˆæ¨èï¼‰
  - ä½¿ç”¨ **Llama 3.2 3B æ¨¡å‹**  
    â¡ï¸ ç¿»è¯‘æ¬¡æ•°æ— é™åˆ¶ï¼Œä½†å¶å°”ä¼šå‡ºç°ç¿»è¯‘é”™è¯¯ã€‚

---

#### **é«˜é…ç½®ç”µè„‘**
- **æµ‹è¯•è®¾å¤‡**ï¼š
  - ğŸ’» é…å¤‡ **Nvidia 4060Ti 16GB æ˜¾å­˜** çš„è®¾å¤‡ï¼Œè¿è¡Œæµç•…ä¸”æ•ˆæœå‡ºè‰²ã€‚
  
- **å¯é€‰è®¾ç½®**ï¼š
  - ä½¿ç”¨ **Llama 3.1 8B æ¨¡å‹**ï¼ˆæ¨èï¼‰  
    â¡ï¸ ç¿»è¯‘è´¨é‡é«˜ï¼Œæ”¯æŒå…è´¹æ— é™åˆ¶ä½¿ç”¨ã€‚
  - ä½¿ç”¨ **ChatGPT API**  
    â¡ï¸ ç¿»è¯‘è´¨é‡æœ€ä½³ï¼Œä½†éœ€è¦ä»˜è´¹ï¼Œå…·ä½“å‚è€ƒ [ChatGPT API æ”¶è´¹è¯´æ˜](https://chatgpt.com/#pricing)ã€‚


## **è®¾ç½®ä¸ä½¿ç”¨æ­¥éª¤**

### **1. ä» GitHub æ‹‰å–é¡¹ç›®**
- å°†é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ã€‚

---

### **2. é…ç½®ç¯å¢ƒï¼ˆWindowsæ“ä½œç³»ç»Ÿï¼‰**
1. ä½¿ç”¨ VScodeï¼Œåœ¨é¡¹ç›®æ–‡ä»¶å¤¹ä¸­æ‰“å¼€ç»ˆç«¯ã€‚
2. è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š
   ```bash
   py -3.11 -m venv .venv
   .venv/Scripts/activate
   ```
3. å®‰è£… GPU ç‰ˆæœ¬çš„ PyTorchï¼š
- ç¡®ä¿å·²å®‰è£… CUDA é©±åŠ¨ã€‚
- ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```
- å¦‚æœæ²¡æœ‰ GPUï¼Œå¯å®‰è£… CPU ç‰ˆæœ¬ï¼š
   ```bash
   pip install torch torchvision torchaudio
   ```
4. å®‰è£…æ‰€æœ‰å…¶ä»–ä¾èµ–åº“ï¼š
- ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š
   ```bash
   pip install -r requirements.txt
   ```
   **æ³¨æ„**ï¼šå®‰è£…è¿‡ç¨‹å¯èƒ½è¾ƒæ…¢ï¼Œéœ€è¦è¶…è¿‡ 2.5GB å­˜å‚¨ç©ºé—´ã€‚å¯ä»¥è¶æ­¤æ—¶é—´å–æ¯å’–å•¡ã€‚

---

### **3. æ”¾ç½®è§†é¢‘æ–‡ä»¶**
- å°†éœ€è¦ç¿»è¯‘çš„è§†é¢‘æ–‡ä»¶æ”¾å…¥ `Video_to_translate` æ–‡ä»¶å¤¹ã€‚
- ç¿»è¯‘å®Œæˆçš„å­—å¹•æ–‡ä»¶ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `Generated_subtitles` æ–‡ä»¶å¤¹ä¸­ã€‚

---

### **4. å¼€å§‹ç¿»è¯‘**
#### **ä½¿ç”¨ DeepL API**
1. æ‰“å¼€ `Autotranslate_Deepl_WhisperX.py` è„šæœ¬ã€‚
2. å°†ç¬¬ 7 è¡Œçš„ `deepL_API` æ›¿æ¢ä¸ºä½ çš„ DeepL API å¯†é’¥ã€‚
3. è¿è¡Œè„šæœ¬å¹¶è€å¿ƒç­‰å¾…ã€‚é¦–æ¬¡è¿è¡Œæ—¶ï¼Œé…ç½®å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚

---

#### **ä½¿ç”¨æœ¬åœ° LLM**
1. ä¸‹è½½å¹¶å®‰è£… **LMStudio** è½¯ä»¶ã€‚
2. åœ¨ LMStudio ä¸­ä¸‹è½½ **Llama-3.2-3B-Instruct-Q8_0** æ¨¡å‹ã€‚
3. æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š  
   *å¼€å‘è€…* â†’ *é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹* â†’ *Llama 3.2 3B Instruct* â†’ *Setting* â†’ *æ£€æŸ¥ Server Port* â†’ *å¯ç”¨ CORS*ã€‚
4. ä¿®æ”¹ `Autotranslate_LLM_WhisperX.py` è„šæœ¬ä¸­çš„ `LLM_MODEL` å˜é‡ä¸ºåœ¨ LMStudio ä¸­ä¸‹è½½çš„æ¨¡å‹åå­—ã€‚
5. è¿è¡Œ `Autotranslate_LLM_WhisperX.py` è„šæœ¬ã€‚

---

### **5. æŸ¥çœ‹è¾“å‡ºç»“æœ**
- å­—å¹•æ–‡ä»¶ä¼šä¿å­˜åˆ° `Generated_subtitles` æ–‡ä»¶å¤¹ã€‚
- å°†è§†é¢‘æ–‡ä»¶å’Œå­—å¹•æ–‡ä»¶æ”¾åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸‹ï¼Œä½¿ç”¨ PotPlayer ç­‰æ’­æ”¾å™¨å³å¯æ’­æ”¾å¸¦å­—å¹•çš„è§†é¢‘ã€‚
ä»“åº“ä¸­æä¾›äº†ç¤ºä¾‹è§†é¢‘â€˜2020S-Qiuqing-Tai-Tiktok.mp4â€™ï¼Œå¯ä»¥ä¾›æµ‹è¯•å‚è€ƒï¼Œæ­¤å¤–åœ¨å­—å¹•ç”Ÿæˆæ–‡ä»¶å¤¹ä¸­æä¾›äº†å¤šç§ç”Ÿæˆçš„ç¿»è¯‘çš„å­—å¹•æ–‡ä»¶ï¼Œå¯ä»¥å‚è€ƒç¿»è¯‘æ•ˆæœã€‚
---
## **Q&A å®‰è£…é—®é¢˜**
### **1. æœ‰ä¸€äº›è­¦å‘Šå¯ä»¥å¿½ç•¥**
```
INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
No language specified, language will be first be detected for each audio file (increases inference time).
Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint D:\Dean-Translator\.venv\Lib\site-packages\whisperx\assets\pytorch_model.bin`
Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.
Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.
```
### **2. å®‰è£…LMStudioåä¸èƒ½å®‰è£…æ¨¡å‹**
å¯ä»¥è¯•è¯•è¿™ä¸ªå·¥å…·[ä¿®æ”¹è„šæœ¬](https://github.com/yuanyang749/lm-studio-fix)
æˆ–è€…æ‰‹åŠ¨ä¿®æ”¹

---

## **Q&A å¸¸è§é—®é¢˜**

### **1. ä¸“ä¸šæœ¯è¯­ç¿»è¯‘å‡†ç¡®å—ï¼Ÿ**
- ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ—¶ï¼Œå‡­å€Ÿä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œä¸“ä¸šæœ¯è¯­ç¿»è¯‘éå¸¸å‡†ç¡®ã€‚
- ä½¿ç”¨ DeepL API æ—¶ï¼Œå¯èƒ½ä¼šå¶å°”å‡ºç°åŒä¹‰è¯ç¿»è¯‘é”™è¯¯ï¼Œä½†æ•´ä½“è¡¨ç°ä¼˜ç§€ã€‚æ¨èä½¿ç”¨å‚æ•°è¾ƒå¤§çš„æ¨¡å‹ä»¥æå‡ç¿»è¯‘è´¨é‡ã€‚

---

### **2. è½¬å½•éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ**
- åˆæ¬¡é…ç½®å¯èƒ½ç¨è€—æ—¶é—´ã€‚ä¹‹åï¼Œè½¬å½•å’Œç¿»è¯‘çš„é€Ÿåº¦ä¸»è¦å–å†³äºç”µè„‘æ€§èƒ½ã€‚
- é…ç½®è¶Šé«˜ï¼Œæ¨¡å‹è¶Šå°ï¼Œå¤„ç†æ—¶é—´è¶Šå¿«ã€‚

---

### **3. å­—å¹•çš„ç²¾ç¡®åº¦å¦‚ä½•ï¼Ÿ**
- ç²¾ç¡®åº¦å–å†³äºä»¥ä¸‹ä¸‰ç‚¹ï¼š
  1. **éŸ³é¢‘è´¨é‡**ï¼šéŸ³é¢‘è¶Šæ¸…æ™°ï¼Œè½¬å½•è¶Šå‡†ç¡®ã€‚
  2. **è½¬å½•æ¨¡å‹æ€§èƒ½**ï¼šå¦‚ä½¿ç”¨ Whisper çš„ *large-v3* æ¨¡å‹ï¼Œè½¬å½•æ•ˆæœæœ€ä½³ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ã€‚
  3. **ç¿»è¯‘æ¨¡å‹è´¨é‡**ï¼šå‚æ•°æ›´å¤šçš„ LLM æ¨¡å‹ç¿»è¯‘æ•ˆæœæ›´å¥½ã€‚

---

### **4. æ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿ**
- æœ¬å·¥å…·æ”¯æŒ Whisperã€DeepL å’Œå¤§è¯­è¨€æ¨¡å‹æ‰€æ”¯æŒçš„æ‰€æœ‰è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€è‹±è¯­ã€æ³•è¯­ã€å¾·è¯­ç­‰ã€‚

---

### **5. æœ€ä½ç¡¬ä»¶è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ**
- ç»æµ‹è¯•ï¼Œåœ¨ MacBook Air M1ï¼ˆ8GB RAMï¼‰å’Œ Nvidia 4060Tiï¼ˆ16GB æ˜¾å­˜ï¼‰ä¸Šè¿è¡Œæµç•…ã€‚
- æ€§èƒ½ä¸ä½äº MacBook Air M1 çš„è®¾å¤‡åŸºæœ¬éƒ½èƒ½è¿è¡Œã€‚

---

### **6. æ”¯æŒå“ªäº›è§†é¢‘æ ¼å¼ï¼Ÿ**
- æ”¯æŒå¤§å¤šæ•°ä¸»æµè§†é¢‘å’ŒéŸ³é¢‘æ ¼å¼ã€‚è¯¦æƒ…å¯å‚è€ƒ Whisper çš„å®˜æ–¹æ–‡æ¡£ã€‚

---
